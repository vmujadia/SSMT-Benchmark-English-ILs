welcome , friends to the sixteenth module of ai . in this particular module we ’ ll be looking at forward and backward state space planning . now , the word ‘ planning ’ is mentioned a few times in the earlier modules and in this particular module , we ’ ll discussing about what do we mean by planning and what are forward and backward state space planning or sometimes called forward planning and sometimes called backward planning . now , an interesting observation probably has revealed this thing to you . we have talked about many problems so far , but then all problems are similar . for example , if i start proving a theorem and in the middle of it , i find that i ' m traveling a wrong path , i may start all over again forgetting about whatever i ' ve done so far . now if i ' m editing a word document and done something , after a while i believe that oh whatever i ' m doing is not correct . i had to do it in some other way , can i start all over again ? you cannot , i have to undo what i have done but i can undo . once i undo that i reach to a point where i want some difference , i can go ahead and do it . i ' m playing chess . i have made some mistake in the second move and i ' m on the fifteenth move . i realize that i ' ve made some mistake in the second move , can i go back and do , i ignore ? neither can i ignore nor can i go back . so there are three different types of problems . one problem is where you ' re exploring a search space . at the middle of it , you just forget about where you were , and start all over again , absolutely fine . in case number two , you can ' t just do that but still you can come back . you can carefully undo the steps that you ’ ve taken and start from the place from where you believe that the error has occurred . in case number three , you don ' t have either of the option . you will have to do whatever you do best from that particular place . you can ' t go back , you can ’ t undo . so , these three type of problems . in many cases obviously , you can see that the problem number three is hardest . now , is it possible for you to convert that hard problem into a little less hard problem ? problem type three into problem two ? the trick is called planning . so , you can plan accordingly that i talked about a case where you are planning your travel or you ' re actually traveling . when you ' re actually traveling , you can ' t go back and restart . okay . that i ’ ve already mentioned . so , actual traveling is the type three problem . you can ' t just do it in fact , sometimes it is type two , you can go back and then retake but that additional burden that you will have to anyway take . so if you start planning in case of chess , the great thing is to plan . see what you will play what the opponent is going to response and what will be your response and so on so forth . based on that you can decide your move now . so , planning helps you improve this thing . anyway , planning is also very , very important for ai programs . planning is important for everything . but planning is very , very useful for planning a solution because when you plan a solution well in advance , especially for problems like problem type three , it ’ ll help you finding out the best path because you will not be in a position to come back and choose a better path at the later stage . before we start planning , we have to have a set of objectives . now , it ' s very important because the plan always is based on the set of objectives that you choose . let us take a very simple example of finding a route of of from one city to the other . now if your objective is to minimize cost , okay , you may choose , you may just pick up cycle , because that ’ s the best option , but that ’ ll take two hours to reach to that city but just just fine is cost effective , solution . if you prefer the user convenience as your objective , you may prefer a luxury car to travel or somebody else would like to look at cost as well as the luxury . you probably would prefer a two - wheeler . so , given a different set of objectives the planning is going to change . not only that , in this case also there may be multiple paths from one city to the other . one is a very short path , but very , very crowded . so , if you want to travel by car that road is not very good . you may prefer a road , which is longer , but with less traffic . so , you ' re planning is going to change based on your objectives . so , this objective is very , very important . so , what are objectives ? in fact , two things you need for the planning . what we want to do is based on our objectives , and second obviously , how we are going to do that the outcome and what is the outcome usually the outcome is sequence of moves ; move one , move two , move three , move four and if you look at the previous module , they are called actions . the agent take actions . okay . basically , action is an implementation of a rule and there is some difference also . let me , talk about the difference right now . when i say that , a chess a has moved two rows in advance , advance to two rows , a piece advance to two rows that is the statement and the rule , okay . so , when the rule is applied the matrix the program changes . but actual movement must take place . so , agent basically is doing that , it moves that piece forward . so , action actually applies the rule . so , it is the implementation of the rule . toh how the rule is implemented in real world is described by action . so , when you want to apply an action , is very similar remember , the rule and action are quite similar . action is just implementation of it . if you want to apply a rule or apply an action in a given state , there are two things needed . the first thing is the prerequisite . if i want to move that piece further , you know that for two for if i want to move that piece , two rows ahead , both rows must not contain any other piece . so , that is the prerequisite . if the prerequisite it does not hold , i cannot generate this rule . so , this action is not possible if the cell on which i want to move is occupied . okay , so that i cannot do . so , the prerequisite must match okay for the actions to apply . in fact , the other important thing . there may be multiple actions available to us . obviously , the first requirement is prerequisite with the action must be applicable in a given state . the second thing is the action may be acceptable , but will that action yield a goal or will it move my state nearer to the goal state ? if it does , then it is better . it ' s more acceptable . so , this is the other thing which we are interested in learning about . so sometimes when you have multiple agents , you will need to choose an action which moves us nearer to the goal state and such actions are known as relevant actions . and when there is a sequence of actions a one , a two , a three , a four , a five then a one then a one applies it generates the prerequisite a two . when a two is applied , a two generates prerequisite for a three and so on . okay . so this is the other requirement and there is an example . for example , an agent which is capable to detect an intrusion , there is a state which says that there is a possibility of denial of service attack . now , you ' ll have to move on from this state and one of the objective of intrusion detection agent is to whenever there is a possibility the objective is to confirm . so it should go to a state called confirm this thing okay . so , it has to move . the other one is to obviously the other state is to communicate to other ids agents running elsewhere in the network , and the third one is to take preventive action . so , these three things are to be done . so , what does it do ? it generates the attack this attack sequence i would say , check for an attack , confirm the attack and then take the preventive action . now , three actions will happen . now , you can see that these three actions will happen only in this sequence . you cannot just take a preventive action and then do something . you have to confirm that attack before . so , without which you cannot work . similar case is described in handout using the possibility of threat of fork in chess . an interesting issue here is that not only we are planning but attacker also is planning . so , when we are doing something the attacker also is doing something . okay . so , what we have to do is to make sure that our plan is such that even if the attacker plans something which thwarts our plan , we ’ ll have to out think the opponent and that is quite an interesting job . so , planning is not always that straightforward . when there is an opponent involved like chess , we have already seen some issues related to that . okay , when the opponent outthink us , we ’ re going to lose , okay . so , that is critical here . there is a sample list of objectives , which are provided on the next slide , which talks about restrict attacker ’ s access to minimum , disallow any system disruption and so on and so forth . now , the idea is that when i ' m planning , these objectives are to be met . but not in all cases , all objectives are possible to be met , you will have to compromise actually . sometimes some objectives are met . sometimes some objectives are not met . there are two alternatives one alternative 90 percent of objectives are met in the other case 80 percent objectives are met , that is quite possible . so , objectives and actions when you try to link . okay , our actions must take i i if you the action must align with our objectives . for example , when we learn that malicious user ’ s ip address , we know that this is malicious users , we can block him . now you can ' t just block an ip address without knowing it . so , that sequence is to be followed . the action can only be taken if there is a prerequisite . the action is only possible if some action is taken to generate that prerequisite . for example , i want to block an attacker ’ s ip address i must execute some action , which get me that ip address . so , that actions are to be listed that way . now , i like when we start from the start state and then we reach to the goal state , we continuously make sure that the objectives are addressed . then there are two different type of constraints . i call them soft and hard constraints . and i take an analogy to explain what these things are and taken it from an academics domain . there is a time table which is being prepared . now one constraints is that a teacher cannot take two classes at the same point of time . now , this is a hard constraint . you cannot violate even if you wish to you cannot violate this . but the other constraint which is called soft constraint , the teacher cannot take two consecutive classes . now , that is a good thing to have that if the teacher has to take two consecutive classes , the delivery will not be to the same level . so , you should try to avoid but it is not that teacher cannot do . so it ’ s called a soft constraint which you can violate once in a while . so , the objectives are also of that type . okay . user convenience for example , user convenience is one objective that i ’ ll have to meet . so , users should not find the network being slow or the server is not accessible and all that but when there is a serious attack happening on network , it is better , you can say that the user convenience take a backseat , you just pick up whatever steps that you need to for thwarting that attack . let the user be little dissatisfied with the service , absolutely fine . so , the objective takes a backseat . so that has to be there . so , this is additional issue . what you supposed to do is to find out hard and soft constraints and follow a path which help you manage or both types of constraints . okay . and based on that you decide what your performance is . and there are many types of planning - simple and detailed planning . so simple planning you don ' t do much . you just narrate and it is done faster but the issue is that detailing may be different and can create an issue . detailed planning is better because you don ' t need to worry while you are implementing but then it takes more time . okay , but then there are other issues . for example one is whether the world is predictable . so you say that i ' ll take this where the world will be like this . and then i take a further step like playing card you say that i play this card and then i ' ll because others will play this , i ' ll have one more card . now , that is not in your hand . the world is not predictable . in case of self - driving car , what is far more predictable ? when you know that when i apply brakes , the speed of the vehicle will reduce , when i apply accelerator , the speed of the vehicle will increase . so , world is predictable and i can predict the outcome of my action in most of the cases . so , world is definitely but then world if you pick up an eight puzzle case world is extremely predictable . i can provide sequence of actions and will clearly tell me whether i ' ll be able to get the answer or not . driving car is not all that predictable and playing card is almost an unpredictable compared to that . the third one , the next one , world is changing while it is being deliberated now you have the situation and you ' re thinking about taking a next step by the time the world changes or not . in case of chess , you have the chess board . the opponent has played his move now you will have to play your own move . but before that , you may think for a while , two minutes , five minutes , ten minutes . the world is not going to change . the world is going to remain the same . unlike that in a self - driving car , the world might change and it is changing actually . if you self - driving car takes five seconds to take a decision it ’ s it ’ s to take a right turn it looks at everything and start the right indicator and everything , it looks at the right mirror and all that and it is working on it and five seconds elapsed in that and can it do it again , like it has to in fact . look at the right mirror just before it plunges it changes the direction . why ? because the world is changing and last five seconds there is probability of some other vehicle come in the view and come in in such a way that if it takes a right turn now it might meet with an accident . so , the world is changing . so that ' s the other issue . the fourth one is whether the world is completely visible or not . sometimes the world is pretty visible . in case of chess the world is completely visible . self - driving car if there is no weather related issue , it is again quite , quite visible . in case of cards , no the world is not visible . you don ' t really know the cards others have . you just can assume you can just predict and work okay . just guess and work . so , for each one of them you require different types of planning and why we are using ? again , let me tell you we are talking about agent based issue , search . agent based search is the same as the other search that we have seen , but it is more explicit and detailed . why ? because when i say that i take a right turn in a graph based that thing , i said right turn and done . but what is the right turn ? in case of agent based it says that the right turn there is an action , there is something which is to be done for taking a right turn . okay . so , that detailed procedure is specified . not only that , there is something which implements that also . okay , for example , it says that it ' ll move the steering wheel in a typical clockwise fashion to do it . so that algorithm , that process is also provided . so , it ' s basically contains a program which implements that action , that rule , okay , which i ' ve already told you that action implements the rule . so , here it is the example . we become more explicit . and that planning when we plan , there are two different ways to plan , one is use planning based on the previous experience that i have . for example , i ’ ll have to plan a conference and i have twenty thirty conferences planned already and what i have learned over the years i may used to organize the next conference that i ' m going to organize . it is quite possible the next time i have to organize an international conference , i ' ve never done that . but then my own experience , partial experience of organizing national conference may help me . i may be given a job which i ' ve never done in past and i ' ll have to give a fresh look and do it all over again . okay , so these two different cases . first one is called case based reasoning . you just whenever you have a problem the agent has a problem to do . okay . that case based reasoning is the other one . the case based reasoning is a very simple way of reasoning . whenever you come across a problem , you just look at in your database . if there is a problem which looks similar , find out the solution that we have taken and apply that . for example , i ' m an intrusion detection agent . what i ' ll do is when i encounter a new attack , i just try to compare this attack with the attacks that i ' ve already seen in past . if i find a similar attack , i ' ll find out how did i do to solve that problem , okay , and then i ' ll follow that process . okay , so but then if a fresh case , i ' ll have to follow a different approach , usually something that i ' ve already done . i apply heuristic , i generate the search space and i move across the search space . hopefully , i ' ll reach to a goal state , okay . case based planning sometimes is known as memory - based planning because the job the is done . now , the next thing that we ' re going to look at is called forward planning or forward state space planning . now , what you do is , you generate the percept sequence which lead to the final state or the actions or sequence of actions to lead to a final state . tow basically , the output of any planning is action sequence , forward or backward . forward , you start from action number one , two , three , four , five in a way that you start from start state and reach to a goal state . in a backward reasoning , you start from action a n , a n minus one and so on such that you start from a final state to just state before and so on till the start state . anyway , this ' s just fine . but then the actions , what will be the order of the action ? in one case , both the actions , action a one must appear , one must execute before action a two . then it ' s called strict ordering . for example , pressing a clutch and changing the gear . now , a one must happen before a two . okay , if you don ' t do that , it ’ ll not happen . so that ' s the strict ordering . but starting an ac and increasing the accelerator value are two parallel things . they are not related to each other , i can order them in any way . so there is no order , un order kind of okay . but then there is a partial ordering . for example , i can start the ac or change the gear but then that only happens after starting the car , without which i cannot do it . so that ' s called partial ordering . there ' s an example i purposefully chosen here and shown on the ppt , which indicates a case where backward reasoning is better or backward state space planning is better . now you can see that initial state and there are multiple states you can generate from initial state . but the from the final state you just look at that , you will realize that it ' s pretty fast for anybody to convert that final state into the four different components of initial state . because it is the problem is , i have designed that way . i ’ ve contrived this example to showcase the usefulness of backward reasoning . in this case , the backward reasoning is quite useful . so , some possible moves are shown here . so , if you begin with the start state , what you can do is , you can there are two things . one pick up a block on the top and put it on the table , or you can put it on some other stack okay . there are five stacks in the beginning . so , there are plenty of options available . the first row talks about putting g on the table , then d on the table , then c on the table . so , the top most items we are putting on the table . so , that ' s one . second one , picking up g and putting it on top of c . next one d , picking up d and putting it on top of somewhere . okay the next one c and putting it top of somewhere . so , you can see that there are plenty of options available . if you move backward , this is not going to happen . move backward , only possible state is putting a back on table . and this is the only possible option and that ' s why it is better because you o… only have one option , okay . when there is one option , you don ' t need to choose and it becomes fast . and that ' s sometimes is called branching factor . branching factor in the forward direction is very high . branching factor in the backward direction is very small . sometimes backward planning is also known as goal directed reasoning . but then you might take some steps . it ' s very important for you to also see whether that ’ s in the right direction . i said i use the word relevant . so , sometimes , you will have to also see whether the move is relevant or not . and this is not heuristic . please understand this , i ' m not talking about heuristic right now . assume that there are some components in the start state . there are some components in this goal state . now , when you move , there are some components which are there in start state , but not in goal state . there are some components in goal state , which are not there in the start state . so , whenever you apply an action suppose , if it adds a component which is there in the goal state and not in the start state , it is good . it deletes a component which is there in the start state and not in the final state , also it is good . now , these two important things indicate that this action will convert the current state into a state which is nearer to goal state than the start state and moves it from start state to goal state . now , that is important here . but , then many a times that is one thing , the relevance of a state that is one thing which i wanted to talk about . the other one , is that the planning is unaware about inter dependency of sub components . then this whatever i ’ ve discussed about is not very important , not very useful . an interesting example is shown in a figure . b is resting on table , a is resting on table , c is resting on top of d , d is resting on table . now what do you want to have is a resting on b , b is resting on c and then d is resting on table . now , what i want to achieve is , on table c , on table d , on ba , on ab . now this is what i want to have from this . now out of this , i say on ab is my first component okay . if i try to achieve that using this , the problem is and you can see that if i place a on top of b , it is nearer to the goal state but it is not goal state . in fact it it is i have to break it apart again . now this is very similar to something that we have seen while we discussed about local and global heuristic functions . though , i ' m going towards goal state , this is not the right way of doing it . so , these are interconnecting goals . so , what i ' m talking about , the relevance and component and all that , these only important or useful if the goals are not inter - dependent . the sub goals are not interdependent . if they are interdependent , i may solve on ab which makes the start date state nearer to the goal state . well , i ' ll not reach to goal state if i do that , okay . the next important thing is to say , in a given case , which one should i use ? should i use a forward reasoning or backward reasoning ? there are many ways to say i ’ ve already talked about the branching factor . the branching factor from start state to end state is more than we should not try from goal state . but then there are other alternatives for example other alternative ways to judge this . for example , if i tell you to go to a place which is unknown to you , from a place which is known , your home , or the option you can start from that unknown place . most of us will prefer to start from that unknown place , because from that unknown place you travel in any direction you reach to a place which is known to you . you can always connect that to your home . so you can always come back . what does it mean ? if you start from your home state and would like to complete , you will have to be there at the place where you want to be . because , in unknown place you don ' t know from where you will reach there . so , without you reaching there you don ' t really know whether you ’ ve reached to a goal state . unlike that , number of options , when you start with an unknown place , is plenty and all of them are start state for you . for example , if you start from the goal state and you reach to say , ten , fifteen , twenty places from where you can reach your home , there are plenty of options available . those fifteen options are possible . so , you can reach to any one of them and you will reach there . so , when such an option and more number of options are available , it is better . second issue is what type of problem is being solved ? for example , if you ' re looking at a big data inquiry that is coming in , look at the amount of posts and let me know whether my product is being sold or not , and whatever . so , what i ' m actually do is to understand what the query is and process backward . because i start from query . i receive a query . i ' ll have to process back . i can ' t start from the beginning . i don ' t really know . or if i have a game playing program like chess , now i cannot start from the final state . i don ' t know what the final state is . there ' s no way for me to know , not only for a conventional state chess program , but you probably have seen many puzzles coming in in everyday newspapers , some chess puzzles . even for those cases , there is a single start state , but there are multiple possible end states . so , it ' s always better to start from the initial state or whatever state that you ' re given . so , point is , in this case , forward reasoning is better . the other one , if explanation facility is required , if the agent is supposed to explain the reasoning . for example , if you take a automated taxi and it says that five hundred rupees , you may ask why five hundred rupees . it has to go back and explain why five hundred rupees . when somebody says that normally i pay only four hundred and you ’ re asking five hundred , why it is so ? the must process back and find out the reasoning for it . so , whenever explanation facility is required , the agent will have to look at the planning process from the goal . okay so the goal directed reasoning or the backward reasoning is very , very important in this case . the other cases , how the solution will be implemented ? for example , if i ' m using prolog to implement the solution . prolog is , by default is a backward chaining mechanism . so , usually you provide your goal , whenever you run up , provide your goal and it will find out the solution . okay , it will always travel in a backward direction . so , if you ' re using a language like prolog , the best option is to move back . if you are using conventional languages , if you use java , c + + , python and all that , the best way to start from the beginning . it doesn ' t mean that you cannot start backwards but natural flow of such programming language is in the forward direction . so , it is better if you do it that way . okay , so that ' s the point . with that we ’ ll come to an end of this discussion , forward versus backward reasoning , which we have seen in the end . there are a lot of things that we have discussed . we looked at planning process , we looked at the components of planning , we looked at what is good planning , we looked at agent based planning , the problems , the world , the environment related issues and so on so forth . and at the end we have compared forward and backward reasoning processes and advantages and disadvantages . looking at all this , we ’ ll now ready for talking about planning process which happens in the real sense programs . okay , so we ' ll be talking about that in the next module . with that we ’ ll conclude .
welcome friends ! we are on the nineteenth module of ai now . we ' ll be talking about game playing algorithms now . game playing is one of the most coveted area of research . and it ' s very , very attractive in terms of revenue generation as well . so a lot of people are working in this direction . game playing itself is an ai domain and we ' ll be talking about why in game playing is an ai domain , what are the problems and how what are the solution . this module and few subsequent modules we ’ ll be looking at the problems and the solutions . okay , we ' ll begin with the introduction about game playing algorithms and some history and a few other prerequisites that one would like to have before embarking on game playing . there is a lot of success and research in commercial field . it ' s very interesting . it ’ s also a very structured domain . what does that mean ? a structured domain because we know start state . every game , we are very clear about the start state . we are very clear about the end state . we ’ ve already seen the example of chess . not only that , the rules are very structured , because people are playing and winning and losing the rules are very , very well defined in most cases . pick up any game and you have very clear idea about those rules . so this is the third characteristic which is very , very important for us . but then they interact with humans . a chess playing program interacts with human , a card playing program interacts with humans , and we want that program to act like human and there ' s where the ai comes into picture . okay , usually you have two problems which are associated with such programs . one is called plausible move generator . take the example of chess . when you look at the chessboard , there may be hundred of moves possible . but will you think of hundred moves ? you only think of few five , six , seven max ten moves , not more . how have you shortlisted those moves ? we don ' t really know . but there is something which we do . over the period of time we are doing it . in fact , there is a structured process . this is not like neurons . there is a structured process . when you start learning it playing chess , you probably every rule is to be reminded like the knight must work this way , a knight rook must proceed this way and so on so forth . so that doesn ' t happen automatically but over a period of time you learn the rules of playing and then generate your own rules , your heuristic rules . you become an expert of playing chess . so there are a lot of heuristics involved like always for example , if there is a player who is very good in playing something called threat of fork then you should learn to be aware of that every time you play chess with him . we ’ ll learn about threat of fork later . in later modules we ’ ll be discussing about that as well . point is we generate those rules . those rules are based on domain knowledge . so based on domain knowledge , we know that out of hundred rules only ten rules , ten moves are important and we only focus , we only compare those rules and choose the best out of it . so that is the first thing that we ’ ll do . the second thing is once we apply those , we and eventually we get some options of the end state would like to compare those end states as well and would like to figure out which end state is better for us and we should choose a path which lead us to that state . okay so again , a kind of heuristic function , it ' s called static evaluation function which is to be applied to those states and looking at the states the static evaluation function ’ s value we ’ ll be able to determine whether a state is good or bad , how good that state is , whether i go ahead with this move , which results into this state or not , and all that . these two things are something which are very , very important for a game playing program but both of them are based on domain knowledge . they are specific for specific game , for chess it is different than some other game . okay but then there are a few things which you can generalize . for example how you move around in state space . now this is not similar to the examples that i ' ve seen there is some difference which we are going to see highlight and see how we can manage . but then that part search part is possible to be managed very easily and it independent of any game okay and we ' ll be looking at that in subsequent modules . advantage of game is obviously there are quite structured rules okay which when whenever you you there you cannot move a piece without the rule okay . for example , you have a rook , in front of it we have a pawn and i can ' t move this rook further unless this pawn is not there . okay , i ' ll have to remove this pawn only then the rule is that i cannot do it . except knight , no other piece can move forward okay . we know this or this is the rule . okay , so , obviously , if you believe that playing a game is simple . chess is hard but if you pick up some simple game . even tic tac toe zero and cross not and cross problem that also is not that simple okay . if you have tried writing a program for it , you probably realize that it requires lot of complexities . and we ' ll be talking about that . one of the major problem with this kind of a problem is that the permutation and combination is too huge and it would be impossible for you to explore the complete game tree in a given time . that is good also , because if you completely explore the game tree , you know exactly which path to follow . and once you reach there you you won . toh you know exactly whether you are going to win or not , and there is no charm in playing that okay . so that ' s in a way a good for laugh the game playing programs . current era , mobile and multi agent games are also in high demand . but again , once we study all this , you realize that this is also going to be a extremely important core component for those games as well . there are some issues , especially one issue which i would like to highlight is that if you have a multi agent game , many complexities related to multi problem , okay , exist . for example , if two cars are racing . now two players cannot race use the same lane to drive their car okay . so that is one thing okay . it ' s called synchronization or park both of them cannot park the car at the same place . so , that is a problem that is something which is additional to what we are going to discuss , but otherwise all conventional game playing issues are there and what we are going to discuss is relevant for all of them . plausible move generator which i talked about which , remember we only as i said we have hundred odd moves and we are only concentrating on ten out of that . now that is also not all that specific . this is also very , very dynamic . dynamic in the sense that it depends on many things first whether it is the starting part or the ending part of a game . usually starting and ending part you don ' t need to worry much about plausible move generators but the middle part you need to . second is criticality of the game . suppose your king is under attack . if the king is under attack , you will have to look at more number of moves . you will just concentrate on ten you will have to look at more number of it is critical and so on so forth . so , there are multiple issues which decide which which kind of plausible move generator should be used and how dense it should generate . remember our discussion about scarce neighborhood and dense neighborhood . here we ’ ll pmg also works in the similar fashion . generally , it uses scarce neighborhood but in a critical condition it goes for the sparse the dense neighborhood function as well . but okay there is one important thing which i would like to also highlight the difference between plausible move generator and a legal move generator . a legal move is legal because you can play it . but plausible move is that move is legal but then will not lead to any significant . we ’ ll not explore it . for example , moving upon further is a legal move but it doesn ' t add any value to our case then what ' s the point in doing it . so plausible move generator will not explore those moves . so legal moves are all possible moves . plausible moves are only those moves which make sense in current situation . there is one more move called winning move . winning move leads us to win . now this is an important category because we want winning moves to be executed out of those plausible move generators . plausible moves may lead to a defeat . we may pick up one of the moves from plausible move generator list but eventually we lose . it is possible . but winning move is something which so , again , these are subsets . so legal moves , then subset of it are plausible moves , subset of which are winning moves . the advantage of plausible move generator is that we can probe much deeper because if we have hundred odd moves to consider and if you have say five minutes or six minutes or some time , we have to have hundred options to explore . we ’ ll not go much deep . but if it reduced to ten , we can go much deeper . commercial chess programs go fifteen , twenty plies deep , twenty levels deep . now this is a huge thing okay , just because they use this plausible move generator . if plausible move generator is not used , this probably would reduce to two three levels or five levels max . one more interesting thing is i was talking about the initial and end part of the game . again , because we have the time constraints in most of the games including chess , we would like to finish that part faster , the initial part and the latter part faster , which we can do by doing a simple trick . we decide that for a given move this is the best move and storing it in the database . we don ' t do anything we don ' t apply search , we don ' t apply heuristic and anything . we know that if the player and if you have seen chess champions playing the initial part of the game , you probably have seen they ' re doing it so fast . they start moving their pieces very , very fast , they would like to do it faster because that in the initial game you know that if opponent is playing this , i ’ ll have to play this and so on so forth . it is not very complicated . so , that thing is stored in a database . so , initial part is stored in the database and that thing is known as a book move okay . that move is stored in the database . so whenever opponent plays that you just match that thing to database and generate . book moves can be huge . but then there is an advantage okay . you just forego all this search and the other thing is going to work . they are quite useful for the beginning and ending part of chess and most of them are used actually . you can ’ t use book moves from the middle part of the game and why because we know that on an average we have thirty five as a branching factor in game and on an average a player plays fifty moves in a game . considering both players you have total thirty five raised to hundred possible moves . now you can ' t explore that search space in in real time . so it is impossible for us . or you can ' t even store that . you can ' t even have the database , to store that large database is almost impossible to achieve . and searching in that database also is too expensive . so , it is just impossible . so you can ’ t use book moves for the middle game . that those are only used for beginning and ending games . okay here as i said , there ' s one more thing heuristic function is used , named as a static evaluation function . what we do is given a point we explore the tree as long as we can and come back and choose a move . for every suppose you have four different alternatives , we try exploring each one , look at what it offers and pick up the best node . this is the general process . now let us talk a bit about the history of game playing programs . cloud shannon talked about game playing programs long back in nineteen fifty and then alan turing also said that it is possible to build a chess playing program and provide a description of that program as well . he didn ' t try building it himself . samuels built checkers program in nineteen sixty . and that checkers program used to play that draught game or the checkers game . that game was and that was a real good example of how one can design a game playing program . that program to learn on its own , learn from the experience . it eventually become so good that it could actually beat the inventor samuel himself . long back in nineteen eighty nine a computer from ibm called deep thought defeated richard levy , champion then . then there was deep blue in nineteen ninety six beaten gary kasparov world champion then , three two by three two margin . they played five games three won by deep thought , two by gary kasparov . and then there was one more program called x3d fritz . that also played with gary kasparov recently in two thousand three , comparatively recently , it is thirteen years before . but it ' s still quite near compared to other . so that drawed level with gary kasparov okay . so that was again , an excellent example . so this is in history . stanford university is so interested , they are running a program , they provide a game playing platform where people can use that platform to build their own games . and then they compete and they till two thousand and five they are running a program , where people can write their program and demonstrate their program and they organize the contest in the end and declare winners of that contest and all that . now a lot of multi agent games are also happening over internet . but then we before we go further , we have to have one more important thing . what are the types of game we are interested in talking about ? there are a few ways to classify games . we have a single person game , solitaire is one example . two - person game is chess . multi person game the car racing is the other example . there so this is one way to differentiate . one more way is whether the game is zero sum . a zero sum game is one ’ s gain is somebody else ' s loss . so if i get something , somebody else will lose . many card games are like that . if i get more hands , other parties will get lesser . okay , so such games are called zero sum games . in some games , it is not so . for example , in car racing , if i collide with somebody else , both of us lose points , okay . my loss is not his gain or some marketing game where it is possible for many of us to profit . if everything is fine , everybody is playing well everybody will profit that ' s an example . the third method is to differentiate is whether the complete information about the game is available . card games you don ' t have complete information . now you you may understand that it is something similar to the discussion about agents that we had before and it is true . complete information games are easier to program . incomplete information games require assumptions to be made and revert those assumptions , okay . that requires that nmrs and all that that we have talked about will come into picture which is more expensive in terms of time as well as storage . okay complete information includes the environment as well . it talks about for example , a car racing game what others are doing , that information is also available with you . if that is available with you only then you will be able to do it well in car racing . if you don ' t know what others are doing , probably you will not be able to do . probably you will be able to collide you will soon collide with somebody else . so complete information games are comparatively better but incomplete information when you don ' t have complete information as i said , you require assumptions and all that . so that makes it little harder . the last way to differentiate is whether it is deterministic . for example if i make a move , can i say that if i make a move i ' ll get this result for sure ? if i ' m not sure , it is not deterministic . if it is sure , it is deterministic . for example , i ' m playing an eight puzzle , i know i move a slide down , i know exactly what will happen or if i have some series of movements , some series of actions , i can calculate the output now and i ' ll have the same output that ’ s not going to change . in case of car driving you cannot have it or even in chess . so these are the points about determine , because in chess it is little more deterministic than car racing . why ? because it only requires the opponents that thing . but your case you know exactly what ' s going to happen . if opponent is not going to play as per your that thing , that part will change . but you know when i ' m placing a piece forward it is going to go forward . okay so that part is there in your hand . so that part is deterministic . in case of car it is not so . in car racing you press the accelerator pedal but might not work because you ’ ve already reached the maximum limit . so you it ' s not all that deterministic . not only that you can ’ t because there are a lot of other players are also involved . so it is not that deterministic . obviously you have to have some assumptions in this case . in case of two player game what we ' re going to do here or for our case we ' ll pick up a two player game . we ’ ll describe all subsequent modules with only assumption of two player games and complete information games . we are not going to look at that part . that part is little more complicated . we require some other as i said those assumptions and that thing . we ’ ll refer to hidden markov model at the end of this thing when we talk about machine , learning how that can be addressed . so we ' ll throw some light on that but otherwise we will not be looking at incomplete information games , okay . two player games can always be extended to multiple agents . only the issue is that synchronization which i talked about that can be added . and also we assume that both of the opponents their goals are completely opposite . so my win is the other person ' s defeat and his win is my defeat , okay . it is not that both of us can win okay that kind of a situation we ’ ll not be talking about . so a game tree is a search tree but then unlike the conventional search tree the layers are different . the first layer if i ' m taking i the first move , i call it max because i ' m going to maximize my chances of winning . next level is played by the opponent and is going to minimize my chances of winning . again , next to next level is my own where i ' m going to maximize my chances of winning . so in that ' s why they are called min - max layer . the max layer is the layer where the player plays and min layer is where the opponent plays . so there may be multiple outcomes . in case of tic tac toe three possible outcomes , win , lose and draw . so tic tac toe is shown on the screen . there are eight different ways of winning . what you want to achieve is three consecutive cells and you mark okay . and if you mark three consecutive cells you winning . so there are eight different ways of winning shown on the screen and also there is one more figure which describes the complete search tree . not complete is also a very small subset of it and we are not exploring every child in every this thing . we are only exploring one child in every layer , obviously because of the space constraint on the screen . but even if there is no space constraint is going to be a huge thing even in this case , but then look at some of the states which are encircled . okay , one two , and there is some something called w , something called l . now you one , you can see that i have two x in the same line but then there is zero played , okay . and the other one you have two x in the line where zero is played elsewhere . now what ’ s the difference between these two ? when i ’ m when two x are in line and when zero is placed next , i ' am preventing that person from winning and when i play zero somewhere else , i ' am not preventing . i ' m making sure that i lose okay . so , that is where it is and you can see that this results in to win or lose based on that information okay . so , w is winning . you can see that all three x in a line so we ' re winning and when three zeros are in in sequence we are losing . so , there is one which is w which is , we would like to reach to a state which is w okay and we would like to avoid states which are l at if we don ' t get w , we may be satisfied with d which is dropped . in this particular case , you can see that it is possible and because it is so such a small program , the problem space , the state itself , i could have i have actually drawn all of them . in case of chess like i can ' t even draw this . but you can easily understand that there are three possible outcomes here - win , lose and draw . so , you can see that there are some cases where i nobody is winning . none of them get that thing in sequence . so , that is a drop and there are three less choices for a second player which you can also see . now with that we ’ ll like to an end of this particular module . what we have seen in this module ? we have just begun talking about game playing . we ' ve looked at some theories and we have looked at plausible move generator , we talked about static evaluation function , we looked at the history of the game playing algorithms , we have looked at a search process which requires minimum and maximum layers , minimum layer minimum the the player is playing the sorry , the minimum layer the opponent is playing , which is going to minimize the player ’ s chance of winning . and with this note we ’ ll end this session , but this is going to be the prerequisite to the subsequent modules where we are actually embarking on learning about game playing in little more depth . thank you . 
