welcome friends to the sixteenth module of ai . in this particular model will be looking at forward and backward state space planning the word planning is mentioned a few times in the earlier module and what are forward and backward observation has revealed this thing to you about many problems so far but then problems are similar . for example , if i start proving a theorem and in the middle of it i find that i am travelling a wrong path i may start all over again forgetting about whatever i have done so far . now if i am editing a word document and done something after a while i believe that whatever i am doing is not correct i had to do it in some other way can i start all over again he cannot i have to undo what i have done but i can undo once i do that i reach to a point where i want some difference i can go ahead and do i am playing chess i have made some mistake in the second move and i am on the fifteenth move i realise that i made some mistake in the second more can i go back and do i can ignore neither can i ignore nor can i go back so there are three different types of problems one problem is where your exploring muscle space at the middle of it you just forget about where you wear start all over again absolutely fine in case number two you cant just do that but still you can come back , you can carefully undo the steps that you have taken . and start from the place from where you believe that the error has occurred do not have either of the option you do whatever you do best from that particular place you cannot undo . these three type of problems in many cases obviously you can see that the problem number three is hardest now is it possible for you to convert that hard problem into little less hard problem type three into problem two the trick is called planning . so , you can plan accordingly that i talked about a case where you are planning your travel or your actually travelling and restart . that i have already mentioned . so , actual travelling is the type three problem you cant just do it in fact sometimes it is type two you can go back and then detect but that additional burden that you will have to anyway take . so , if you start planning in case of chess , the great thing is to plan see what you will play what the opponent is going to response and what will be your response and so on and so forth based on that you can decide your move now . so , planning helps you . improve this thing anyway planning is also very very important for ai programs planning is important for everything but planning is very useful for planning a solution because when you plan a solution , especially for problems like problem type 3 , it help you . finding out the best path because you will not be in a position to come back and choose a better path at a later before we start planning we have to have a set of objectives now it is very important because the plan always is based on the set of objectives that you choose . let us take a very simple example of finding a route from one city to the other . if your objective is to minimize cost . ok you may choose you may just pick up cycle because that is the best option but the might take two hours to reach to that city but just just fine is cost effective solution if you prefer the user convenience as your objective you may prefer a luxury car to travel or somebody else would like to look at cost as well as the luxury probably would prefer a two wheeler so , given a different set of objective the planning is going to change . in this case also there may be multiple paths from one city to the other one is a very short path but very very crowded so if you want to travel by car thus that road is not very good you may prefer a road . which is longer , but with less traffic . so , your planning is going to change . based on your objective so this objective is very very important so what are objectives in fact two things you need for planning what we want to do is based on our objectives and second how we do that outcome and what is the outcome usually the outcome sequence of moves move one move two move and if you look at the previous module they are called actions the agent take actions . basically action is an implementation of a and there is some difference also let me talk about the difference right now . when i say that a chess has moved to rows in advance to two rows . a piece advance to two rows that is the statement and the rule ok so when the rule is applied the matrix changes but actual movement must take place . so , agent basically is doing that . it moves that piece forward so action actually applies the rule how the rule is implemented in real world is described by action is very similar remember rule and action are quite similar action is just implementation if you want to eat apply a rule or apply an action in a given state . if i want to know that you know that for two if i want to move that piece two rows ahead . both rows must not contain any other piece so that is the prerequisite . if the prerequisite does not hold i cannot generate this rule . so , this action is not possible if and the cell on which i want to move is occupied . so , that i cannot do . so the prerequisite must match okay for the actions to apply in fact the other important thing there may be multiple actions available to us . obviously , requirement is prerequisite with the action must be applicable in a given state . the second the action may be acceptable , but will that action yield a goal or will . it move my state nearer to the goal state . if it does then it is better . so , this is the other thing which we are interested in learning about . when you have multiple agents you will need to choose an action which moves us nearer to the gold standard such actions are known as relevant actions and when there is a sequence of actions a one a two a three a four a five then a 1 when a 1 applies it generates the prerequisite for a 2 when a 2 is applied a 2 generates prerequisite for a 3 and so on . so , this is the other requirement and there is an example . for example , an agent which is capable to detect an inclusion . there is a state which says that there is a possibility of develop service attack . now you will have to move on from the state and one of the objective of intrusion detection agent is to whenever there is a possibility the object . is to confirm . so , it should go to a state called confirm this thing . so , it has to move . the other one is to obviously , the other state is to communicate to other ids agents running elsewhere in the and the third one is to take preventive action . so , these three things have to be done so , what does it do it generates the attack sequence? attack response sequence i would check for an attack confirm the attack and then take the preventive action though these three actions will happen . now , you can see that these three actions will happen only in this sequence . you cannot just take a preventive action and then do something you have to confirm so , without which you cannot work . case is described in handout . using the possibility of threat of fork in chess . issue here is that not only we are planning but attacker also is planning . so when we are doing something the attacker also is doing something okay so when what we have to do is to make sure that our plan is such that even if the attacker plans something which towards our plan will have to out thing that is quite an interesting job so , planning is not always that straight forward when there is an opponent involved like chairs we have already seen some issues related to that . ok when the opponent out fingers we are going to lose . so , that is critical here . there is a sample list of objectives which are provided . on the next slide which talks about risky attackers access to minimum disallow any system disruption and so on and so forth . now , the idea is that when i am planning these objectives are to be met but not in all cases all objectives are possible to be match , you will have to compromise actually . objectives are made . sometimes some objectives are not met there are two alternatives one alternative ninety so , objectives and actions when you try to link . our actions must . take action must align with our objective for example when we learn that malicious users we can block him now you cant just block an ip address without knowing it so that sequence is to be followed the action can only be taken if some action is taken to generate the for example , i want to block an attackers ip address i must execute some action . which get me that ip address . so , that actions are to be listed that way . now , i like when you start from the start state and when we reach to the goal state we continuously make sure that the objectives are addressed . then , there are two different type of constraints i call them soft and hard constraints . and i take an analogy to explain what these things are . taken a strong an academic domain there is a time table which is being prepared now man one constraints is that a teacher cannot take two classes at the same point of time you cannot violate this , but the other constraint which is called soft constraint . teacher cannot take to consecutive classes the delivery will not be to the same level so we should try to avoid but it is not that teacher cannot do so is called a soft constraint which you can violet once in a while so the objectives are also of that type user convenience for example , user convenience is one object that i have to meet . so , user should not find the network being slow or the server is not accessible and all but when there is a serious attack happening on network it is better . you can say that the user convenience take a back seat you just pick up whatever steps that you need to for thoughting that attack let the user be a little dissatisfied with the service absolutely fine . the objective takes a back seat . so , that has to be there . so , this is additional issue what you are supposed to do is to find out hard and soft constraints and follow a path which help you manage both types of constraints ok . and based on that you decide what you performance is and there are many types of planning . and it is done faster but the issue is that detailing may be different and kind might detailed planning is better because you dont need to worry while implementing but then it takes more time ok but then there are other issues for example one is whether the bolt is predictable . so , you say that i will take this . the world will be like this and then i take a further stab . card . you say that i play this card and then i will because others will play this i will play one more card now that is not in your hand the world is not predictable in case of self driving card what is far more predictable when you know that the speed of the vehicle will increase so world is predictable and i can predict the outcome of my action in most of the cases so world is extremely predictable . i can provide sequence of actions and will clearly tell me whether i will be able to get the answer or not triving is not all that predictable and playing cards almost in unpredictable compared the third one world is changing well it is being deliberated now you have the situation now you are thinking about taking a next step by the time the world changes or not in case of chess . you have the chessboard the opener is played is moon you have to play your own move but before that you may think for a while two minutes five minutes the world is not going to change the world is going to remain the same unlike that in a self driving car the world might change and it is changing actually if the self driving cortex 5 seconds to take a decision it is wants to take a right turn and it looks at everything and start the right indicator and everything looks at the right mirror and all that is working on it five second elapse in that . do it again like it has to in fact look at the right mirror just before it plunges . changes the direction why because the world is changing and last five seconds probability of some other vehicle come in the view and come in in such a way that if it takes a right and now it might make with an accident . so , the world is changing so that is the other issue . the fourth one is whether the world is completely visible or not sometimes , the world is pretty visible in case of chess the world is completely visible cell driving car if there is no weather related issue . is again quite visible in case of cars no . the world is not visible . you do not really know the cards others have , you just can assume . you can just predict and work . so , for each one of them you require different types of planning . and why we are using again , let me tell you why we are talking about agent based . issues search search is the same as the other search that we have seen but is more explicit and detailed why because when i say that i take a right turn . in a graph based that thing i said right turn and done but what is right turn in case of agent based he says that the right turn there is an action there is something which is to be done for taking a right turn so that detailed procedure is specified not only that there is something which implements that also . ok for example it says that it will move the steering well in a typical clockwise fashion to do it . so , that algorithm is also provided . so , it is basically contains a program which implements that action ok . so , which i have already told you that action implements the rule . so , here it is the example . we become more explicit . and that planning when we do plan , one is use planning based on the previous experience plan a conference and i have twenty conferences planned already . what i have learnt over the years i may use to organize the next conference that i am going to organize . it is quite possible that next time i have to organize an international conference , but experience of organizing national conference may help me i may be given a job which i never done in past and have to give a fresh who can do it all over again so , these two different cases , first one is called case based reasoning , you just whenever you have a problem to do case based reasoning is the other one . the case based reasoning is a very simple way of whenever you come across a problem if there is a problem with look similar find out the solution that we have taken . for example , i am an intrusion detection agent , what i do is when i counter a new attack i just try to compare this attack with the attacks already seen in past if i find a similar attack i will find out how did i do to solve that problem and then i will follow that process . so , but then if a fresh case i will have to follow a different approach . something that i have already done i apply heuristic i generate the search space and i move across the search space . hopefully i will reach to a goal state ok . case based planning sometimes is known as memory based planning because the job did is done in my . the next thing that we are going to look at is called forward . planning or forward states place planning . now , what you do is? the percept sequence which lead to the final state or the actions or sequence of actions to lead to a final state so , basically the output of any planning is action sequence forward or backward . forward you start from number one two three four five in a way that you start from start and reach to a goal in a backward reasoning you start from x and a and a n minus one and so on . such that you start from a final state to just state before and so on till the start state . anyway this is just fine , but then the actions what will be the order of the action . in one case both the actions . action a 1 must appear 1 must execute before . before x and a two then it is called strict ordering for example , pressing a clutch . and changing the gear . now , a 1 must happen before a2 if you do not do that will not happen so that is strict ordering . but starting an ac and increasing the accelerator value are two parallel things they are not related to each other i can order them in any way so , there is no order on order kind of , but then there is a partial ordering . for example , i can start the ac or change the gear . but then that only happens after studying the car without which i cannot do it . so , that there is an example i purposefully chosen here . and so on on the ppt which indicates a case where backward reasoning is better or backward states place planning is better . now , you can see that initial state under a multiple states you can generate from initial state , but from the final state you just look at that you will realize that its pretty fast for anybody to convert that final state into the four different components of initial state . because it is the problem is i have designed that way i have contrived this example to showcase the usefulness of backward region in this case the backward reasoning is quite useful . so , some possible moves are shown here . so , if you begin with the start state what you can do is you there are two things one pick up a block on the top and put it on the table . or you can put it on some other . there are 5 stacks in the beginning . the plenty of options available . so , the first row talks about putting g on the table , then d on the table . so , the top most item we are putting on the table . picking up g and putting it on top of c . d picking up d and putting it on top of somewhere ok . next one see and putting a top of somewhere . you can see that there are plenty of options available if you move backward this is not move backward only possible state is putting a back on table . is the only possible option and that is why it is better because you only have one option when there is one option you do not need to choose and it becomes fast . and that sometimes is called branching factor , branching factor in the forward direction is very high branching factor in the backward direction is very small . sometimes backward planning is also known as goal directed reasoning . but then you might take some steps , it is very important for you . to also see whether that is . that is in the right direction . i said the word relevant . so , sometimes you will have to also see whether the move is relevant or not and this is not heuristic please understand this i am not talking about heuristic right now . assume that there are some components in the start state , there are some components in the goal state now when you move there are some components which are there in start state , but not in goal state . there are some components in goal state which are not there in the start state . so , whenever you apply an action , suppose if it adds a component which is there in the it is good , it deletes a component which is there in the start state and not in the final state . then also it is good . now , these two important things indicate that this action will convert the current state . into a state which is nearer to goal state then the starts and moves it from . moving it from start state to goal state that is important here . many a times that is one thing the relevance of a state that is one thing i wanted to talk about the other one is that the planning is unaware about inter dependency of sub components . whatever i have discussed about is not very important not useful . an interesting example is shown in a figure . b is resting on table a is resting on table c is resting on top of b is resting on table what i want to achieve is on table c on table d on b a on . now , this is what i want to have from this now out of this . i say on ab is my first component . if i try to achieve that using this the problem is you can see that if i place a on top of b it is nearer to the whole state , but it is not goal state . in fact , it is . i have to break it apart again . this is very similar to something that we have seen while we discussed about local and global heuristic functions though i am going towards goal state this is not the right way of doing it . so , these are inter connecting so , what i am talking about the relevance and component and all that is only important or useful the goals are not interdependent . so , sub goals are not interdependent . if they are interdependent i may solve on ab which makes the start state nearer to the goal state . i will not reach to goal state if i do that ok . the next important thing is to say in a given case which one should i use . should i use a forward reasoning or backward reasoning . there are many . ways to say we have already talked about the branching factor from starts at to end side is more than we should not try from gold state for example , if i tell you to go to a place which is unknown to you from a place which is known . your home or the option you can start from that unknown place . most of us will prefer to start from that unknown place . from that unroll place you travel in any direction you reach to a place which is known to you you can always connect that to your home so you can always come back . what does it mean if you start from your home state would like to complete you have to be there at the place where you want to be there so without you reaching there you dont really you know whether you have reached to a goal state unlike that number of options when you start with an unknown + is plenty and all of them are start state for you . for example , if you start from . the goal state and you reach to 10 , 15 , 20 plus is from where you can reach your home so , you can reach to any one of them and you will reach there . such an options and more number of options are available it is better . second issue is what type of problem is being solved for example if you are looking at a big data enquiry that is coming in look at the amount of post and let me know whether my product is being sold . or non whatever . so , what i am actually do is to understand what the queries and process backward because i start from query i receive a query i will have to process back i cant start from the beginning i dont know or if i have again playing program like chess now i cannot start from the final state not only for a conventional state chess program but you probably have seen many puzzles coming in in everyday newspapers some chess puzzles even for those cases there is a single start state , but there is a multiple . possible and state so is always better to start from the initial state whatever state that you will given . so , point is in this case you have forward reasoning is better the other one if explanation facility is required . if the agent is supposed to explain the reasoning for example , if you take up . automated taxi and he says that 500 rupees you may ask why 500 rupees it has to . go back and explain why it must process back and find out the reasoning for it so , whenever exploration facility is required , the agent will have to look at the planning process from the gold . ok so the goal director reasoning or the backward reasoning is very very important in this the other cases how the solution will be implemented for example if i am using prolog to implement the solution by default is a backward chaining mechanism so usually you provide your goal whenever you run up provide your goal and find out the solution . always travel in a backward direction . so if you are using a language like prolog the best option is to move back to using conventional languages if you use java c plus plus python and all that the best way to start from the beginning it doesnt mean that you cannot start the backwards but natural flow of such programming languages in the forward direction . so , it is better if you do it that so , that is the point with that we will come to an end of this discussion . forward versus backward reasoning which we have seen in the end the lot of things that we have discussed . we looked at planning process we looked at the components of planning we looked at what is good planning we looked at agent based planning the problems the world the environment related issues and so on and so forth and at the end we have compared looking at all this will we are now ready for talking about planning process which happens in . the real sense programs will be talking about that in the next module that will conclude thank you . 
welcome friends . we are on the 90th module of ai now . will be talking about game playing algorithms now game playing is one of the most coveted area of research . very attractive in terms of revenue generation as well . so lot of people are working in this game playing itself is an ai domain and we will be talking about why game playing is an ai and how are the solutions and some history and a few other prerequisites that one would like to have before embarking on game playing is lot of success in research in commercial field . its very interesting is also very structured domain what does that mean a structured domain because we know start state . every game we are very clear about the start set we are very clear about the end state we have already seen the example of chess not only that the rules are very structured are playing and losing the rules are very very well defined in most cases pick up any and you have very clear idea about those rules so , this is the third characteristic which is very very important for us . but then they interact with humans chess playing program interact human humans and we want that program to act like human there is where the a i comes into picture . usually you have two problems which are associated with such programs one is called glossible look at the chessboard . hundreds of most possible will you think of hundred only think of you five six seven moves not more how have you shortlisted those moves we dont really know but there is something which we do over the period of time we are doing it is a structured process is not like neuron is a structured process when you start learning playing chess you probably every rule is to be reminded like the day night must work this way night rook must proceed this way and so on so forth . that doesnt happen automatically but over a period of time you learn the rules are playing and then generate your own rules . you become an expert of playing chess so there are lot of heuristics involved like always . for example , if there is a player who is very good in playing something call threat of work then you should learn to be aware of that every time you play chess with him will learn about threat of for later in later modules will be discussing about that as well . we generate those rules . those rules are based on domain knowledge . so based on domain knowledge we know that hundred rules are important and only compare and choose the best out of it so thats the first thing that we do the second thing is once we apply those we thing and eventually we get some options of the end state would like to compare those end states as well i am like to figure out which end state is better for us and we should choose a path which lead us to that state . so , again a kind of heuristic function . it is called static evaluation function , which is to be applied to those states and looking at the states the static evaluation functions value will be able to determine whether a state is good or bad how good that state is whether it go ahead with this move which results into the state or not and all that . which are very very important for again playing program but both of them are based on domain knowledge . they are specific for specific game . for chairs it is different than some other game ok but then there are few things which you can generalize for example how you move around in state space now , this is not similar to the . that i have seen there is some difference which we are going to see highlight and see how we manage but then that part search part is part possible to be managed very easily and it independent of any game . and we will be looking at that in subsequent modules . advantage of game is obviously there are quite structured rules . ok which when whenever you you there you cannot move a piece without rule . for example , we have a rook in front of it we have a and i cannot move this route further unless this ponus not there ok i will have to remove this phone only then the rule is that i cannot do it except night no other piece can move forward we know this . this is the rule . so , obviously , if you believe that playing a game a simple . chess is hard but if you pick up some simple game even tic tac toe 0 and cross not and cross problem . that also is not that simple if you have tried writing a program for it requires lot of complexities and will be talking about that problem with this kind of a problem is that it would be impossible for you to explore that is good also because if you come at completely explore the game tree you know exactly which path to follow . and once you reach there you you want you know exactly whether you win or not and there is no charm in playing that ok . thats in a way good for game playing programs current era mobile and multi agent games are also in high demand but again once we study all this you realize that this is also going to be a extremely important core component for those games as well there are some issues specially #um one issue which i would like to highlight is that if you have a multi agent game many complexities related to multi problem okay exist . for example , if two cars are racing . players cannot race use the same land to drive their car okay so that is one thing ok its called synchronization or part both of them cannot park the car at the same place so that is a problem which is additional to what we are going to discuss . otherwise all conventional are there and what we are going to discuss is relevant for all of them plausible move generator which i talked about which remember we only as i said we have hundred odd moves and are only concentrating on ten out of that now that is also not all that specific this is also very very dynamic it depends on the starting part of a game usually starting and ending part you dont need to worry much about the middle part second is criticality of the game suppose you are king is under attack if the king is under attack you will have to look at the more number of moves you cannot just concentrate on ten look at more number of is critical and so on and so forth . so , there are multiple issues with decide with which kind of possible move generator should be used and how dense . it should generate . remember , i discussed about the scars neighborhood and dense neighborhood . here we will pmg also works in the similar fashion . generally it uses cars neighborhood but in a critical condition it goes for the dense neighborhood function as well but there is one important thing which i would like to also highlight the difference between plausible move generator and legal move generator and legal move is legal . because you can play it but plausible moon that is legal but then will not lead to anything significant we will not explore it for example moving upon further is a legal move but it doesnt add any value to our case then whats so plausible move generator will not explore the moves . so , legal moves are all possible moves . possible moves are only those moves which makes sense in current situation there is one more move call winning move meaning no leads us to win now this is an important category because we want to winning moves to be executed out of those possible moves generator . possible moves may lead to a defect we may pick up one of the moves from classical move generator list but eventually we lose it is possible but winning move is something which so again these are subsets or legal moves then subset of it are possible modes subset of which are meaning modes the advantage of possible move generator . is that we can prove much differ because if we have hundred or most to consider and if you have six minutes sometime we have to have hundred options to explore will not go much dip . but if it reduced to ten we can go much deeper . commercial chess programs go fifteen twenty price day twenty fifteen twenty labels deep and this is a huge thing . just because they use this possible is not use this probably would reduce to two three levels one more interesting thing is i was talking about the uni initial and end part of the game again because we have the time constraints in most of the games including chess . we would like to finish that part faster the initial past and the later part faster which we can do by doing a simple trick . we decide that for a given move this is the best move . and storing that we dont do anything we dont apply sir we dont apply heuristic and anything we know that if the player and if you have seen a chess champions playing the initial part of the game you probably have seen that doing it so fast there start moving their pieces very very fast they would like to do it faster because that in initial game you know that is playing this i have to play this and so on so forth it is not very complicated so that thing is stored in the database so initial part is stored in the database and that thing is known as a book move . ok that move is stored in the database . so whenever open place that you just match that thing the database in general can be huge but then there is an advantage ok you just forgo all the search and the other thing is going to work are quite useful for the beginning and ending part of chess and most of them are used you cant use book moves from the middle part of the game and why because we know that . on an average we have thirty five as a branching factor in game and on an average player place fifty moves in again considering both players you have total 35 raise to 100 possible . moves now you cant explore that search space in real time so , it is impossible for us . or you cant even store that you cant even have the database to store that you large database is almost impossible to achieve . and searching in the database also is too expensive . so it is just impossible so you cant use book moves for the middle game you that those are only used for beginning and ending games ok here as i said there is one more thing heuristic function is used . named as a static evaluation function what we do is . given a point we explore the tree as long as we can and come back and choose . ammu for every suppose you have four different alternatives we try exploring each one look at what it offers and pick up the base note this is the general process let us talk a bit about the history of game playing programs cloud shannon talked about game playing programs long back . in nineteen fifty and then alan turing also said that this possible to build a chess playing program and provide description of that program as well he didnt try building it himself seminals build checkers program in nineteen sixty . and that checkers program used to play that drought game or checkers game that game was and that was a real good example of how one can design a chair again playing program that program one of the great ability of that program to learn on its own . learn from the experience it eventually become so good that could actually britti the inventor seminal himself long back in nineteen eighty nine a computer from ibm called dip thought defeated richard levy champion olympic champion then there was a dip blue in 1996 . between gary kaspara world champion then three two by three two margin they played five games three one by dip thought two by gary casparov and then there was one more program called x three d freeds that also played with gary gaspora recently in two thousand three comparatively recently is thirteen years before but it is still quite near compared to so that broad drawn label with very kasparo okay so that was again an excellent example so this is an history stanford university is so interested they are running a program they provide a game playing platform where people can use that platform to build their own games and then they compete and they till still two thousand five they are running a program where people can write their program and demonstrate their program and they the organize the contest and the end . and declare winners of that contest and all that . there are lot of multi agent games are also happening over internet but then we before we go further we have to have one more important thing what are the types of game we are interested in talking about there are a few ways to classify games we have a single person game solitair is one example . two person game is chess multi person game multi player game the car is saying is the other example there so this is one way to differentiate . one more way is whether the game is 0 sum a zero sum gave me is once again is somebody else is lost so if i get something somebody else will lose . many card games are like that . if i get more hands then other parties will get lesser ok so such games are called zero some games it is not so . for example in car i think if i collide with somebody else both of us lose points ok mind loss is not his game or some marketing game where it is possible for many of us to profit if everything is fine everybody is playing well everybody will profit thats an example the third method is to differentiate is whether the complete information about the game is available . card games , you do not have complete information . now you may understand that is something similar to the discussion about agents that we had before and it is true complete information games are easier to program . incomplete information games require assumptions to be made and reward those assumptions ok that we have talked about will come into picture which is more expensive . in terms of time as well as storage ok complete information includes the environment as well . for example , a car ac game what others are doing that information is also available with if that is available with you only then you will be able to do it well if you dont know what others not be able to do probably you will be able to collide you will soon collide with somebody so , complete information games are . comparatively better but incomplete information when you do not have complete information so that makes it to harder the last way to differentiate is whether it is deterministic . for example , if i make a move . can i say that if i make a move i will get this result . for sure if i am not sure it is non deterministic if it is sure it is deterministic for example i am playing an eight puzzle i know i move a slide down i know exactly what will happen or if i have some series of actions i can calculate the output now and i will have the same output in case of card driving you cannot have a door or even in chess so these are the points about determining because in chairs little more deterministic then car racing why because its only requires the opponents that thing but your case you know exactly whats going to happen to play as per your that thing that part will change but you know when i am placing a piece forward it is going to go forward okay so that part is there in your hand so that part is deterministic in case of car it is not so in car issing you press the accelerator pedal but might not work where you already reached the maximum limit so you its not all the deterministic not only that you cant because there are lot of other players are also involved . so , it is not . that deterministic obviously you have to have some assumptions in this case in case of two player game what we are going to do here are for our case will pick up a two player game will describe all subsequent modules with only assumption of two player games and complete information game you are not going to look at that part that part is little more complicated we require some other as i said those assumptions and that thing to hidden model at the end of this thing when we talk about machine learning how that can be addressed . will throw some light on that , but otherwise we will not be looking at incomplete information games ok . can always be extended to multiple agents only the issue is that can be added and also be assume that both of the opponents their goals are completely opposite so my win is the other persons in the feet and his when is my defect it is not that both of us can win . that kind of a situation we will not be talking about so , a game tree is a search tree . but then unlike the conventional search tree the layers are different the first layer if i am taking the first move i call it max because i am going to maximize my chances of winning next level is played by the opponent and is going to minimize my chances of winning again , next to next level is my own where i am going to maximize my chances of meaning . so , in that is why they are called minmax layer the max layer is . the layer where the player plays and mean layer is where the opponent plays . so there may be multiple outcomes in case of ticket or three possible outcomes win loose and draw . so , tic tacto is expected . shown on the screen there are eight different ways of winning . what do you want to achieve three consecutive cells and you mark ok and if you mark three consecutive cells you winning so , there are 8 different ways of winning shown on the screen and also there is one more figure which describes the complete search tree not complete is . a very small subset of it and we are not exploring every child in every this thing we are only exploring one child in every layer . obviously of the space constraint on the screen , but even if there is no space constraint is going thing even in this case , but then look at some of these states which are encircled . ok one two and there is something called w something called l now you this one you can see that i have two x in the same line , but then there is zero played . the other one you have two x in the line where zero is played elsewhere what is the difference between this two when i am x are in line and when zero is placed next i am preventing that person from winning i place zero somewhere else i am not preventing i am making sure that i lose that is where it is and you can see that this results into win or lose based on that information so , w is winning you can see that all three x in a line . so , we are winning . and when three zeros are in sequence we are losing . one which is w which is . like to reach to a state which is w ok and we would like to avoid states which are at if we dont get w we may be satisfied with d which is draw in this particular case you can see that it is possible and because it is so such a small program . the problem space the state itself . i could have actually drawn all of them in case of chess like i cant even draw this but you can easily understand that there are three possible outcomes here we lose and so , you can see that . there are some cases where i nobody is meaning i know none of them get that thing in sequence so that is a draw and there are three list choices for a second player which you can also see now with that will like to welcome to an end of this particular module what we have seen in this module we have just begin talking about game playing we have looked at some theories and we have looked at . we talked about static evaluation function we looked at the history of game playing algorithms we have looked at process which requires minimum and maximum layers . minimum layer the the player is playing the sorry , the minimum layer the opponent is playing , which is the declared chance of winning and with this note will end this session but this is going to be the prerequisite to the subsequent modules where we are actually embarking on learning about game playing in little more depth .
